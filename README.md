# Тестовое задание на FastAPI

Минимальны сервис с RAG, с поиском по локальной БЗ (`dataset/kb/*.md`), генерация ответа LLM.
Выполнил Звягинцев Иван

# Инструкции по запуску

## Локально

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload
```

Запрос через curl:
```bash
curl -X POST http://127.0.0.1:8000/ask \  -H "Content-Type: application/json" \  -d '{"question": "Как мне получить доступ к VPN?"}'
```

## Docker
Сервис доступен на порту :8000

```bash
docker compose up --build
```

## Переменные окружения
- `KB_DIR` -- путь к директории с `.md` файлами (в проекте: `dataset/kb`)
- `TOP_K` -- количество документов для контекста (по умолчанию: 3)
- `MODEL_NAME` -- имя модели (в проекте: оффлайн-заглушка)

## Метрики и логирование
По каждому запросу согласно заданию логируются: `request_id`, `latency_ms`, `model`, `tokens_{prompt,completion,total}`, `top_k`, `source_ids`. Формат -- JSON через `structlog`.

## Тесты / Lint / CI
```bash
pytest
ruff check .
```

GitHub Actions: установка зависимостей > `ruff` > `pytest` > сборка Docker-образа.

## Оценка (локально)
После запуска сервиса:
```bash
python eval/eval_client.py eval/queries.jsonl
```

Получатся `hit_rate`, `p50/p95` и суммарные токены согласно заданию.


## Почему TF-IDF
Легко и быстро реализовывается для MVP, не нужно обучать модель (у меня в проекте заглушка), быстрые индексация и поиск, хорошо подходит для коротких текстов (наш случай, база знаний очень маленькая)


## Про LLM
В этом задании мною использована примитивная **заглушка** (без сетевых вызовов). В точку входа можно легко вставить любую LLM (Gemini/GigaChat/локальная) -- достаточно заменить реализацию в `app/llm.py` и отредактировать переменные окружения с включением туда ключей. Заглушка простым наивным способом рассчитывает и передаёт токены.
Выбор был сделан в пользу заглушки, так как у меня сейчас в распоряжении нет оплаченных ключей, заливать на публичный гитхаб я бы в любом случае их не стал. Сейчас она просто выступает в роли времеянки.

## Асинхронность
Пути FastAPI (`/health`, `/ask`) я выполнил по инерции как `async def`. Внутренние CPU-операции (ретривер и генерация заглушки) оффлоадятся через `asyncio.to_thread`, чтобы не блокировать event loop. Это мой стандартный modus operandi, когда пишу API.


## Кэширование
Реализован простой ин-мемори кэш ответов (`CACHE` в main.py), чтобы ускорить повторные запросы с одинаковым вопросом. Инвалидируется только при перезапуске сервиса (что логично).


## Фоллбеки при отсутствии нужных библиотек
В случае, если structlog или sklearn окажутся недоступными, то предусмотрен фоллбек на чистый питон.
У меня был кейс, когда sklearn конкретно под TF-IDF нельзя было использовать, поэтому в коде есть его вариант, который не опирается на импорт. Когда есть возможность я предпочитаю не изобретать велосипед, но код уже есть -- использовал.


## Путь развития проекта дальше
Помимо замены заглушки LLM на действующую модель
Далее согласно соотношению труд/полезность -- ранжирование, отдельный эндпоинт для SSE и векторный ретривер с гибридным поиском (эмбеддинги + pgvector, объединить с BM25)
Следующим бы шагом добавил prometheus для метрик.
Далее нужно смотреть устраивает ли заказчика проект, нужно ли работать над оптимизацией, добавлять динамический top-k, переписывать запросы, нужен ли чанкер, ingestion пайплайн, добавлять какие-то фильтры для запрета определённых вопросов и т.д.
В текущем виде минимально функционирующий сервис для ответа на вопросы из небольшого датасета (то естькак MVP) при добавлении нормальной LLM модели и с переходом на pgvector на мой взгляд реализован.
